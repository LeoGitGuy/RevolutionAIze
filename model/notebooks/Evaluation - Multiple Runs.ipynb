{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#Basic Imports\n",
    "import os,sys\n",
    "os.chdir(\"..\")\n",
    "\n",
    "from tqdm import tqdm,trange\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report,confusion_matrix, ConfusionMatrixDisplay\n",
    "import torch\n",
    "import pandas\n",
    "\n",
    "from datasets.ssl_dataset import SSL_Dataset\n",
    "from datasets.data_utils import get_data_loader\n",
    "from utils import get_model_checkpoints\n",
    "from utils import net_builder\n",
    "from utils import clean_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path to the runs to load\n",
    "csv_folder = \"/home/gabrielemeoni/project/SSLRS/test\"\n",
    "folder = \"/scratch/fixmatch_results/nr_of_labels/eurosat_rgb/\"\n",
    "sort_criterion = \"numlabels\" # Accepted net, numlabels\n",
    "seed_wanted = 1 # Seed wanted (the others will be filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints, run_args = get_model_checkpoints(folder)\n",
    "if os.name == 'nt':\n",
    "       [print(_.split(\"\\\\\")[1]) for _ in checkpoints];\n",
    "else:\n",
    "       [print(_.split(\"/\")[1]) for _ in checkpoints];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for checkpoint, args in zip(checkpoints,run_args):\n",
    "    print(\"------------ RUNNING \", checkpoint, \" -----------------\")\n",
    "    print(args)\n",
    "    args[\"batch_size\"] = 256\n",
    "    args[\"data_dir\"] = \"./data/\"\n",
    "    args[\"use_train_model\"] = False\n",
    "    args[\"load_path\"] = checkpoint\n",
    "    if args[\"seed\"] == seed_wanted:\n",
    "        checkpoint_path = os.path.join(args[\"load_path\"])\n",
    "        checkpoint = torch.load(checkpoint_path,map_location='cuda:0')\n",
    "        load_model = (checkpoint[\"train_model\"] if args[\"use_train_model\"] else checkpoint[\"eval_model\"])\n",
    "        _net_builder = net_builder(args[\"net\"],False,{})\n",
    "        _eval_dset = SSL_Dataset(name=args[\"dataset\"], train=False, data_dir=args[\"data_dir\"], seed=args[\"seed\"])\n",
    "        eval_dset = _eval_dset.get_dset()\n",
    "        net = _net_builder(num_classes=_eval_dset.num_classes, in_channels=_eval_dset.num_channels)\n",
    "        net.load_state_dict(load_model)\n",
    "        if torch.cuda.is_available():\n",
    "            net.cuda()\n",
    "        net.eval()\n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "        eval_loader = get_data_loader(eval_dset, args[\"batch_size\"], num_workers=1)\n",
    "        label_encoding = _eval_dset.label_encoding\n",
    "        inv_transf = _eval_dset.inv_transform\n",
    "    \n",
    "        \n",
    "        print(\"------------ PREDICTING TESTSET -----------------\")\n",
    "        \n",
    "        images, labels, preds = [],[],[]\n",
    "        with torch.no_grad():\n",
    "            for image, target in tqdm(eval_loader):\n",
    "                image = image.type(torch.FloatTensor).cuda()\n",
    "                logit = net(image)\n",
    "                for idx,img in enumerate(image):\n",
    "                    images.append(inv_transf(img.transpose(0,2).cpu().numpy()).transpose(0,2).numpy())\n",
    "                preds.append(logit.cpu().max(1)[1])\n",
    "                labels.append(target)\n",
    "        labels = torch.cat(labels).numpy()\n",
    "        preds = torch.cat(preds).numpy()\n",
    "        test_report = classification_report(labels, preds, target_names=label_encoding, output_dict=True)\n",
    "        test_report[\"params\"] = args\n",
    "        results.append(test_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "big_df = pd.DataFrame()\n",
    "pd.set_option('display.max_columns', None)\n",
    "for result in results:\n",
    "    params = result[\"params\"]\n",
    "    df = pd.DataFrame(result)\n",
    "    df.drop(list(params.keys()),inplace=True)\n",
    "    df.drop([\"support\",\"recall\",\"precision\"],inplace=True)\n",
    "    for key,val in params.items():\n",
    "        df[key] = val\n",
    "    df = df.set_index(\"dataset\")\n",
    "    big_df = big_df.append(df)\n",
    "print(big_df)\n",
    "small_df = clean_results_df(big_df, folder,sort_criterion)\n",
    "small_df.to_csv(csv_folder + \"_test_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
